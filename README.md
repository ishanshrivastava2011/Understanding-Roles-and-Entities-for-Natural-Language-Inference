# Enhancing natural language inference using new and expanded training data sets and new learning models

This repository contains the code and link to the datasets created and used in this work for all the experiments mentioned in the paper. 

## [Link to the Dataset and Model](https://drive.google.com/drive/folders/16gVgY_69luIv5JTvBbWKbGpKpr6uZjkA?usp=sharing)

This is the Google Drive link for the best performing model and all the datasets that were used for conducting all the experiments discussed in the paper including the two new datasets that we created.

## Requirements

- Python 3
> It's best to create a virtual environment
```
conda create -n allennlp_editable python=3.7
```

- Allen NLP 
> Install [Allennlp](https://github.com/allenai/allennlp#installing-from-source) from source. Follow the steps mentioned in this [link](https://github.com/allenai/allennlp#installing-from-source) to install allennlp from source. 

### (Optional) To create NER features for a new dataset
- Spacy
- Stanford Core NLP (stanford-ner-2018-10-16)

## Training Configs

These are the configuration files as required by the AllenNLP framework
